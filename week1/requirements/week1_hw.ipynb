{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Week 1. Titanic Classification with DistilBERT + XGBoost 과제\n",
        "\n",
        "Week 1. 과제로는 `week1_lecture.ipynb` 코드 예제를 참고하여, `kaggle`에서 titanic 데이터를 받아 데이터 전처리를 진행한 후 학습해 자신의 모델을 만든 후, ipynb 파일과 결과 csv파일을\n",
        "\n",
        "- ***hw1_학번_이름.ipynb***\n",
        "- ***submission.csv***\n",
        "\n",
        "실전데이터사이언스 스터디 repository week1 폴더에 커밋하시면 됩니다.!\n",
        "\n",
        "https://github.com/a2ran/prac_ds\n",
        "\n",
        "타이타닉 데이터 전처리 예시는 다음과 같습니다.\n",
        "\n",
        "1. Filling out missing (NaN) Values\n",
        "2. VIF으로 분산이 높은 column 제거 or 수정\n",
        "3. train_test_split 으로 train/val 나누는 과정 수정\n",
        "4. \"bert-base-uncased\" 이외 다른 BERT 모델을 사용해 Embedding하기\n",
        "5. \"xgboost\" 이외 다른 머신러닝 알고리즘 사용\n",
        "\n",
        "참고할만한 example은 다음과 같습니다.\n",
        "\n",
        "\n",
        "https://github.com/minsuk-heo/kaggle-titanic/blob/master/titanic-solution.ipynb\n",
        "\n",
        "`Optional` :\n",
        "\n",
        "https://github.com/mrdbourke/your-first-kaggle-submission/blob/master/kaggle-titanic-dataset-example-submission-workflow.ipynb\n",
        "https://github.com/agconti/kaggle-titanic/blob/master/Titanic.ipynb\n"
      ],
      "metadata": {
        "id": "UXgpncMecp3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기본 제공 코드"
      ],
      "metadata": {
        "id": "gb8IaA0yc0sI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Kaggle에서 데이터 받아오기"
      ],
      "metadata": {
        "id": "sW5DIEUzq2QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## pip install ~ : 패키지 다운로드\n",
        "## -q : 로그 메세지 출력 X\n",
        "\n",
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "s8HYJnxLd1Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Kaggle에서 데이터를 받아오기 위해서는 Authenticator Token인 \"kaggle.json\"이 있어야 합니다.\n",
        "## 자세한 내용은 영상을 참고하세요\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "## Drive에 kaggle.json을 업로드한 경로를 적으시면 됩니다. ex) (/content/drive/MyDrive/study_session/kaggle.json)\n",
        "! cp /content/drive/MyDrive/\"\"your_directory\"\" ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "zU1akXDEd16i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 공모전 이름\n",
        "competition_name = \"titanic\"\n",
        "\n",
        "# 공모전 다운로드 to local environment\n",
        "! kaggle competitions download -c {competition_name}\n",
        "\n",
        "# {competition_name}이름의 폴더에 zip 파일 압축해제\n",
        "! unzip {competition_name + \".zip\"} -d {competition_name}\n",
        "\n",
        "# 드라이브 확인을 완료했으므로 드라이브 mount를 해제합니다.\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "cz89QBpNd3ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 데이터 전처리"
      ],
      "metadata": {
        "id": "7nwhJIagq4r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## GPU 활성화\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "eQKII2dXd94v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('titanic/train.csv')\n",
        "test = pd.read_csv('titanic/test.csv')"
      ],
      "metadata": {
        "id": "wG5QXTVlqFiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Your Codes"
      ],
      "metadata": {
        "id": "cA1k8GIsqGAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 데이터 임베딩"
      ],
      "metadata": {
        "id": "lIMOGIriq8oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "!pip install -q transformers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class Encode_with_BERT:\n",
        "    def __init__(self):\n",
        "        ## Huggingface에서 BERT 모델을 받아옵니다.\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.model = AutoModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "    ## 입력한 `texts`을 768 길이의 숫자벡터로 특징을 추출합니다.\n",
        "    def extract(self, texts: List[str]):\n",
        "        features = np.zeros((len(texts), 768), dtype = np.float16)\n",
        "        for index, text in enumerate(tqdm(texts)):\n",
        "            tokenized_text = self.tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "            model_output = self.model(**tokenized_text)[0].detach().cpu()\n",
        "            features[index, :] = model_output.numpy().mean(axis=1)\n",
        "\n",
        "        return features"
      ],
      "metadata": {
        "id": "oHQMzZtsqGZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 임베딩한 데이터 머신러닝 알고리즘으로 분류"
      ],
      "metadata": {
        "id": "nidbrVoDrAjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "extractor = Encode_with_BERT()\n",
        "scaler = StandardScaler()\n",
        "classifier = XGBClassifier(use_label_encoder = False)\n",
        "\n",
        "## XGBoost 학습\n",
        "train_labels = [int(_) for _ in y_train.values] ## [0,1,1, ...]\n",
        "texts = [\", \".join(str(_)) for _ in X_train.values] ##[[0,1,male],[2,1,female]...]\n",
        "train_features = scaler.fit_transform(extractor.extract(texts)) ## to (len(texts), 768) 숫자벡터\n",
        "classifier.fit(train_features, train_labels) ## XGBoost train\n",
        "\n",
        "## Prediction with Test Data\n",
        "answer = [int(_) for _ in y_val.values]\n",
        "texts = [\", \".join(str(_)) for _ in X_val.values]\n",
        "preds = classifier.predict(scaler.transform(extractor.extract(texts)))\n",
        "\n",
        "## 모델 예측의 정밀도 측정!\n",
        "accuracy = accuracy_score(answer, preds)\n",
        "print(f'\\naccuracy : {accuracy*100:.2f}%')"
      ],
      "metadata": {
        "id": "YeY-YoSYqfaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. csv로 저장 + kaggle에 submit"
      ],
      "metadata": {
        "id": "ji495QvArD8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "ids = test['PassengerId'].values\n",
        "texts = [\", \".join(str(_)) for _ in test.iloc[:, 1:].values]\n",
        "preds = classifier.predict(scaler.transform(extractor.extract(texts)))\n",
        "\n",
        "with open('submission.csv', \"w\") as to_file:\n",
        "    csvwriter = csv.writer(to_file, delimiter=\",\", quotechar='\"')\n",
        "    csvwriter.writerow([\"PassengerId\", \"Survived\"])\n",
        "    for id, pred in zip(ids, preds):\n",
        "        csvwriter.writerow([id, pred])"
      ],
      "metadata": {
        "id": "nZoUVbXkqgth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub = pd.read_csv('submission.csv')\n",
        "sub.head(5)"
      ],
      "metadata": {
        "id": "WNAg-AJvqiNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}